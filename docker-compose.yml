version: '3.8'

services:
  llama_server:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "${LLAMA_SERVER_PORT}:8080"
    volumes:
      - ${LLAMA_MODEL_PATH}:/models
    command:
      [
        "-m",
        "models/${LLAMA_MODEL_FILE}",
        "-c",
        "512",
        "--host",
        "0.0.0.0",
        "--port",
        "8080"
      ]

  streamlit_app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "${STREAMLIT_APP_PORT}:8501"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "--fail",
          "http://localhost:8501/_stcore/health"
        ]
    environment:
      - PYTHONPATH=${PYTHONPATH}:.
    volumes:
      - ./.streamlit:/app/.streamlit
      - ./grobid:/app/grobid
    entrypoint:
      [
        "streamlit",
        "run",
        "streamlit_app.py",
        "--server.port=8501",
        "--server.address=0.0.0.0"
      ]

  grobid:
    image: lfoppiano/grobid:0.8.0
    ports:
      - "${GROBID_PORT}:8070"
    init: true
    ulimits:
      core: 0
    command: [ "server" ]
